# -*- coding: utf-8 -*-
"""Churn_host.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/136-4oHzb-zDtv6YjJzLRig8oIay6waIw
"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
import seaborn as sns 
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline,make_pipeline

df = pd.read_csv('/content/Churn_Modelling.csv')

df.shape
# gives rows and columns/ features

df.info()

# we will now eliminate first 3 columns as they seem of no use
df.drop(columns=['RowNumber', 'CustomerId', 'Surname','Geography'], inplace=True)

df.head(2)

from sklearn.preprocessing import OneHotEncoder
trf1 = ColumnTransformer([
    ('ohe',OneHotEncoder(sparse=False,handle_unknown='ignore'),[1])
],remainder='passthrough')

# df=df.reindex(['CreditScore','Gender','Age','Tenure','Balance','NameOfProducts','HasCrCard','IsActiveMember','EstimatedSalary','Exited'],axis=1)

df.isnull().sum()

from sklearn.model_selection import train_test_split
X = df.drop('Exited',axis=1,inplace=False)
y= df['Exited']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=41)
# X = df.iloc[:, :-1].values
# y = df.iloc[:, -1].values.reshape(-1,1)
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

from sklearn.ensemble import RandomForestClassifier
rf_model = RandomForestClassifier(n_estimators=2, min_samples_split=3,min_samples_leaf=1,max_features='auto',max_depth=3,bootstrap=True)

pipe = make_pipeline(trf1,rf_model)
pipe.fit(X_train, y_train)

y_pred = pipe.predict(X_test)

from sklearn.metrics import accuracy_score
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

from sklearn.metrics import f1_score
f1_score(y_test, y_pred, average='weighted')

# from sklearn.model_selection import RandomizedSearchCV
# from pprint import pprint
# # Number of trees in random forest
# n_estimators = [int(x) for x in np.linspace(start = 2, stop = 200, num = 10)]
# # Number of features to consider at every split
# max_features = ['auto', 'sqrt']
# # Maximum number of levels in tree
# max_depth = [int(x) for x in np.linspace(start=3, stop=10, num = 1)]
# # Minimum number of samples required to split a node
# min_samples_split = [2, 3, 5,7]
# # Minimum number of samples required at each leaf node
# min_samples_leaf = [1, 2, 4]
# # Method of selecting samples for training each tree
# # Create the random grid
# random_grid = {'n_estimators': n_estimators,
#                'max_features': max_features,
#                'max_depth': max_depth,
#                'min_samples_split': min_samples_split,
#                'min_samples_leaf': min_samples_leaf}
# pprint(random_grid)

# # Use the random grid to search for best hyperparameters
# # First create the base model to tune
# rf = RandomForestClassifier()
# # Random search of parameters, using 3 fold cross validation, 
# # search across 100 different combinations, and use all available cores
# rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)
# # Fit the random search model
# rf_random.fit(X_train, y_train)

# rf_random.best_params_
# output :
# {'n_estimators': 2,
#  'min_samples_split': 3,
#  'min_samples_leaf': 1,
#  'max_features': 'auto',
#  'max_depth': 3}

X_test.head(2)

yhat = pipe.predict(pd.DataFrame([[818,'Male',36,4,0.00,2,1,1,8037.03]], columns=['CreditScore','Gender','Age','Tenure','Balance','NumOfProducts','HasCrCard','IsActiveMember','EstimatedSalary']))

print(yhat)

import pickle

pickle.dump(pipe, open('churnHost1.pkl', 'wb'))